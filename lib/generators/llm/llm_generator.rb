class LlmGenerator < Rails::Generators::Base
  source_root File.expand_path('templates', __dir__)

  desc "Generate LLM service with clean async callback support"

  class_option :skip_service, type: :boolean, default: false, desc: "Skip service file generation"
  class_option :skip_job, type: :boolean, default: false, desc: "Skip job file generation"
  class_option :skip_model, type: :boolean, default: false, desc: "Skip model file generation"
  class_option :skip_migration, type: :boolean, default: false, desc: "Skip migration file generation"

  def create_service_file
    return if options[:skip_service]
    template 'llm_service.rb.erb', 'app/services/llm_service.rb'
  end

  def create_job_file
    return if options[:skip_job]
    template 'llm_job.rb.erb', 'app/jobs/llm_job.rb'
  end

  def create_model_file
    return if options[:skip_model]
    template 'llm_request.rb.erb', 'app/models/llm_request.rb'
  end

  def create_migration_file
    return if options[:skip_migration]
    template 'migration.rb.erb', "db/migrate/#{timestamp}_create_llm_requests.rb"
  end

  def update_application_yml
    llm_config = <<~YAML

      # LLM Service Configuration generated by llm generator
      LLM_BASE_URL: '<%= ENV.fetch("CLACKY_LLM_BASE_URL", '') %>'
      LLM_API_KEY: '<%= ENV.fetch("CLACKY_LLM_API_KEY", '') %>'
      LLM_MODEL: '<%= ENV.fetch("CLACKY_LLM_MODEL", '') %>'
      # LLM Service Configuration generated end
    YAML

    # Update application.yml.example
    add_llm_config_to_file('config/application.yml.example', llm_config)

    # Update application.yml if it exists
    add_llm_config_to_file('config/application.yml', llm_config)
  end

  def show_usage_instructions
    say "\n" + "=" * 80
    say "LLM Generator completed successfully!", :green
    say "=" * 80

    say "\nðŸ“ Configuration:"
    say "  Environment variables added to config/application.yml.example"
    say "  Configure these in your config/application.yml:"
    say "    LLM_BASE_URL     - API endpoint (e.g., https://api.openai.com/v1)"
    say "    LLM_API_KEY      - Your API key"
    say "    LLM_MODEL        - Model name (e.g., gpt-4o-mini, deepseek-chat)"

    say "\nðŸš€ Usage Examples:"

    say "\n  1. Synchronous call:"
    say "     result = LlmService.call(prompt: 'Explain quantum computing')"
    say "     if result.success?"
    say "       puts result.data[:content]"
    say "       puts result.data[:tokens]"
    say "     end"

    say "\n  2. Asynchronous call (fire and forget):"
    say "     result = LlmService.call_async(prompt: 'Analyze this data...')"
    say "     request = result.data[:request]  # LlmRequest object"

    say "\n  3. Async with block callback:"
    say "     LlmService.call_async(prompt: 'Hello AI') do |request, error|"
    say "       if error"
    say "         Rails.logger.error \"LLM Error: \#{error.message}\""
    say "       else"
    say "         puts \"Result: \#{request.response}\""
    say "         # Process request.response here"
    say "       end"
    say "     end"

    say "\nðŸ“š Next Steps:"
    say "  1. Run migrations: rails db:migrate"
    say "  2. Configure your API keys in config/application.yml"
    say "  3. Try it in console: rails c"
    say "     > LlmService.call(prompt: 'Say hello!')"

    say "\n" + "=" * 80 + "\n"
  end

  private

  def timestamp
    Time.now.utc.strftime("%Y%m%d%H%M%S")
  end

  def add_llm_config_to_file(file_path, llm_config)
    if File.exist?(file_path)
      content = File.read(file_path)

      unless content.include?('# LLM Service Configuration generated by llm generator')
        append_to_file file_path, llm_config
        say "Added LLM configuration to #{File.basename(file_path)}", :green
      else
        say "LLM configuration already exists in #{File.basename(file_path)}, skipping...", :yellow
      end
    else
      say "#{File.basename(file_path)} not found, skipping...", :yellow
    end
  end
end
