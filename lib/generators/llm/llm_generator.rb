class LlmGenerator < Rails::Generators::Base
  source_root File.expand_path('templates', __dir__)

  desc "Generate LLM service with streaming and blocking API support"

  class_option :skip_service, type: :boolean, default: false, desc: "Skip service file generation"
  class_option :skip_job, type: :boolean, default: false, desc: "Skip job file generation"

  def create_service_file
    return if options[:skip_service]
    template 'llm_service.rb.erb', 'app/services/llm_service.rb'
  end

  def create_job_file
    return if options[:skip_job]
    template 'llm_stream_job.rb.erb', 'app/jobs/llm_stream_job.rb'
  end

  def create_llm_message_validation_concern
    template 'llm_message_validation_concern.rb.erb', 'app/models/concerns/llm_message_validation_concern.rb'
  end

  def update_application_yml
    llm_config = <<~YAML

      # LLM Service Configuration generated by llm generator
      LLM_BASE_URL: '<%= ENV.fetch("CLACKY_LLM_BASE_URL", '') %>'
      LLM_API_KEY: '<%= ENV.fetch("CLACKY_LLM_API_KEY", '') %>'
      LLM_MODEL: '<%= ENV.fetch("CLACKY_LLM_MODEL", 'gemini-2.5-flash') %>'
      # LLM Service Configuration generated end
    YAML

    # Update application.yml.example
    add_llm_config_to_file('config/application.yml.example', llm_config)

    # Update application.yml if it exists
    add_llm_config_to_file('config/application.yml', llm_config)
  end


  def show_usage_instructions
    # Display generated files content
    unless options[:skip_service]
      service_file = 'app/services/llm_service.rb'
      say "\n"
      say "ğŸ“„ Generated service (#{service_file}):", :green
      say "â”" * 60, :green
      File.readlines(service_file).each_with_index do |line, index|
        say "#{(index + 1).to_s.rjust(4)} â”‚ #{line.chomp}"
      end
      say "â”" * 60, :green
      say "âœ… This is the latest content - no need to read the file again", :cyan
    end

    unless options[:skip_job]
      job_file = 'app/jobs/llm_stream_job.rb'
      say "\n"
      say "ğŸ“„ Generated job (#{job_file}):", :green
      say "â”" * 60, :green
      File.readlines(job_file).each_with_index do |line, index|
        say "#{(index + 1).to_s.rjust(4)} â”‚ #{line.chomp}"
      end
      say "â”" * 60, :green
      say "âœ… This is the latest content - no need to read the file again", :cyan
    end

    say "\n"
    say "LLM Generator completed successfully!", :green

    say "\nğŸ“ Configuration:"
    say "  Environment variables added to config/application.yml.example"
    say "  Configure these in your config/application.yml:"
    say "    LLM_BASE_URL     - API endpoint (e.g., https://api.openai.com/v1)"
    say "    LLM_API_KEY      - Your API key"
    say "    LLM_MODEL        - Model name (e.g., gpt-4o-mini, deepseek-chat)"

    say "\nğŸš€ Usage (Streaming via ActionCable):"
    say "     LlmStreamJob.perform_later("
    say "       channel_name: \"chat_\#{user_id}\","
    say "       prompt: 'Explain quantum computing',"
    say "       system: 'You are a helpful assistant'"
    say "     )"

    say "\nğŸ“š Next Steps:"
    say "  1. Configure your API keys in config/application.yml"
    say "  2. Use LlmStreamJob with ActionCable for real-time streaming"

    say "\nğŸ¤– AI Assistant Note:"
    say "  When storing LLM messages, include LlmMessageValidationConcern."
    say "  Don't validate role & content yourself - the concern handles it."
  end

  private

  def add_llm_config_to_file(file_path, llm_config)
    if File.exist?(file_path)
      content = File.read(file_path)

      unless content.include?('# LLM Service Configuration generated by llm generator')
        append_to_file file_path, llm_config
        say "Added LLM configuration to #{File.basename(file_path)}", :green
      else
        say "LLM configuration already exists in #{File.basename(file_path)}, skipping...", :yellow
      end
    else
      say "#{File.basename(file_path)} not found, skipping...", :yellow
    end
  end
end
