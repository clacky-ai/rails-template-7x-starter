class LlmGenerator < Rails::Generators::Base
  source_root File.expand_path('templates', __dir__)

  desc "Generate LLM service with streaming and blocking API support"

  class_option :skip_service, type: :boolean, default: false, desc: "Skip service file generation"
  class_option :skip_job, type: :boolean, default: false, desc: "Skip job file generation"

  def create_service_file
    return if options[:skip_service]
    template 'llm_service.rb.erb', 'app/services/llm_service.rb'
  end

  def create_job_file
    return if options[:skip_job]
    template 'llm_job.rb.erb', 'app/jobs/llm_job.rb'
  end

  def update_application_yml
    llm_config = <<~YAML

      # LLM Service Configuration generated by llm generator
      LLM_BASE_URL: '<%= ENV.fetch("CLACKY_LLM_BASE_URL", '') %>'
      LLM_API_KEY: '<%= ENV.fetch("CLACKY_LLM_API_KEY", '') %>'
      LLM_MODEL: '<%= ENV.fetch("CLACKY_LLM_MODEL", '') %>'
      # LLM Service Configuration generated end
    YAML

    # Update application.yml.example
    add_llm_config_to_file('config/application.yml.example', llm_config)

    # Update application.yml if it exists
    add_llm_config_to_file('config/application.yml', llm_config)
  end


  def show_usage_instructions
    say "LLM Generator completed successfully!", :green

    say "\nüìù Configuration:"
    say "  Environment variables added to config/application.yml.example"
    say "  Configure these in your config/application.yml:"
    say "    LLM_BASE_URL     - API endpoint (e.g., https://api.openai.com/v1)"
    say "    LLM_API_KEY      - Your API key"
    say "    LLM_MODEL        - Model name (e.g., gpt-4o-mini, deepseek-chat)"

    say "\nüöÄ Usage Examples:"

    say "\n  1. Streaming call (default, real-time response):"
    say "     LlmService.call(prompt: 'Explain quantum computing') do |chunk|"
    say "       print chunk  # Prints each chunk as it arrives"
    say "     end"

    say "\n  2. Blocking call (wait for full response):"
    say "     result = LlmService.call_blocking(prompt: 'Hello AI')"
    say "     if result.success?"
    say "       puts result.data[:content]"
    say "     end"

    say "\n  3. Background job with callback:"
    say "     class MyCallback"
    say "       def initialize(result, error)"
    say "         @result, @error = result, error"
    say "       end"
    say "       def call"
    say "         # Process @result or @error"
    say "       end"
    say "     end"
    say ""
    say "     LlmJob.perform_later(prompt: '...', callback_class: 'MyCallback')"

    say "\nüìö Next Steps:"
    say "  1. Configure your API keys in config/application.yml"
    say "  2. Try streaming: LlmService.call(prompt: 'Hi!') { |c| print c }"
  end

  private

  def add_llm_config_to_file(file_path, llm_config)
    if File.exist?(file_path)
      content = File.read(file_path)

      unless content.include?('# LLM Service Configuration generated by llm generator')
        append_to_file file_path, llm_config
        say "Added LLM configuration to #{File.basename(file_path)}", :green
      else
        say "LLM configuration already exists in #{File.basename(file_path)}, skipping...", :yellow
      end
    else
      say "#{File.basename(file_path)} not found, skipping...", :yellow
    end
  end
end
